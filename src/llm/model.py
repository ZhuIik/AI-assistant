from pathlib import Path
import torch
from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel
from src.config import BASE_MODEL, LORA_PATH


def load_pipeline():
    print("üîπ –ó–∞–≥—Ä—É–∂–∞–µ–º –±–∞–∑–æ–≤—É—é –º–æ–¥–µ–ª—å Gemma...")
    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)

    model = AutoModelForCausalLM.from_pretrained(
        BASE_MODEL,
        device_map="auto",
        torch_dtype=torch.float16
    )

    print(f"üîπ –ü—Ä–æ–≤–µ—Ä—è–µ–º LoRA –∞–¥–∞–ø—Ç–µ—Ä –ø–æ –ø—É—Ç–∏:\n{LORA_PATH}")

    adapter_config = LORA_PATH / "adapter_config.json"
    if not adapter_config.exists():
        raise FileNotFoundError(
            f"\n‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω adapter_config.json!\n"
            f"–ü—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã–π –ø—É—Ç—å: {adapter_config}\n"
            f"–ü—Ä–æ–≤–µ—Ä—å LORA_PATH –≤ config.py"
        )

    print("üîπ –ü–æ–¥–∫–ª—é—á–∞–µ–º LoRA –∞–¥–∞–ø—Ç–µ—Ä...")
    model = PeftModel.from_pretrained(model, str(LORA_PATH))

    print("üîπ –°–æ–±–∏—Ä–∞–µ–º –ø–∞–π–ø–ª–∞–π–Ω...")
    pipe = pipeline(
        "text-generation",
        model=model,
        tokenizer=tokenizer,
        device_map="auto",
        torch_dtype=torch.float16
    )

    print("‚úÖ –ú–æ–¥–µ–ª—å —Å LoRA –≥–æ—Ç–æ–≤–∞!")
    return pipe
