# scripts/chat_local.py
from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel
import torch

BASE = "google/gemma-2-2b-it"                 # –∏–ª–∏ "google/gemma-2-9b-it"
ADAPTER = "outputs/gemma_lectures_lora_v1/checkpoint-3"  # –ø—É—Ç—å –∫ —Ç–≤–æ–∏–º –æ–±—É—á–µ–Ω–Ω—ã–º –≤–µ—Å–∞–º

print("üîπ –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å...")
tokenizer = AutoTokenizer.from_pretrained(BASE)
model = AutoModelForCausalLM.from_pretrained(BASE, device_map="auto", torch_dtype=torch.float16)
model = PeftModel.from_pretrained(model, ADAPTER)

pipe = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    device_map="auto",
    torch_dtype=torch.float16
)

print("‚úÖ –ú–æ–¥–µ–ª—å –≥–æ—Ç–æ–≤–∞! –ú–æ–∂–µ—à—å –∑–∞–¥–∞–≤–∞—Ç—å –≤–æ–ø—Ä–æ—Å—ã:\n")

while True:
    prompt = input("‚ùì –í–æ–ø—Ä–æ—Å: ")
    if prompt.lower() in ["exit", "quit", "stop"]:
        print("üõë –ó–∞–≤–µ—Ä—à–µ–Ω–æ.")
        break
    output = pipe(prompt, max_new_tokens=250, do_sample=True, temperature=0.7)[0]["generated_text"]
    print("\nüí¨ –û—Ç–≤–µ—Ç –º–æ–¥–µ–ª–∏:\n", output)
    print("-" * 80)
