# scripts/finetune_lora.py
import os
import torch
import torch.nn as nn
from datasets import load_dataset
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    BitsAndBytesConfig,
    TrainingArguments,
)
from peft import LoraConfig, get_peft_model
from trl import SFTTrainer

# === 1. –ü—É—Ç–∏ –∏ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã ===
DATA_PATH = "data/datasets/lectures_v1.jsonl"
OUTPUT_DIR = "outputs/gemma_lectures_lora_v1"
MODEL_NAME = "google/gemma-2-2b-it"

# === 2. –ó–∞—â–∏—Ç–∞ –æ—Ç .to() –æ—à–∏–±–æ–∫ (–ø–∞—Ç—á –¥–ª—è bitsandbytes) ===
old_to = nn.Module.to
def safe_to(self, *args, **kwargs):
    if "bitsandbytes" in str(type(self)).lower():
        return self
    return old_to(self, *args, **kwargs)
nn.Module.to = safe_to

# === 3. –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç ===
dataset = load_dataset("json", data_files=DATA_PATH)

def formatting_func(example):
    text = f"User: {example['instruction']}\nAssistant: {example['output']}"
    return [text]  # –≤–∞–∂–Ω–æ –≤–æ–∑–≤—Ä–∞—â–∞—Ç—å —Å–ø–∏—Å–æ–∫!

# === 4. –¢–æ–∫–µ–Ω–∞–π–∑–µ—Ä ===
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
tokenizer.pad_token = tokenizer.eos_token

# === 5. –ö–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ (QLoRA, 4-bit) ===
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.float16   # ‚Üê –±—ã–ª–æ bfloat16
)


print("üîπ –ó–∞–≥—Ä—É–∂–∞–µ–º Gemma 2 9B –≤ 4-–±–∏—Ç–Ω–æ–º —Ä–µ–∂–∏–º–µ...")
model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    quantization_config=bnb_config,
    device_map="auto",
    low_cpu_mem_usage=True,
    torch_dtype=torch.float16           
)


# === 6. –ù–∞—Å—Ç—Ä–æ–π–∫–∞ LoRA ===
lora_config = LoraConfig(
    r=8,
    lora_alpha=32,
    lora_dropout=0.05,
    target_modules=["q_proj", "v_proj"],
    task_type="CAUSAL_LM",
)
model = get_peft_model(model, lora_config)

# === 7. –ê—Ä–≥—É–º–µ–Ω—Ç—ã –æ–±—É—á–µ–Ω–∏—è ===
args = TrainingArguments(
    output_dir=OUTPUT_DIR,
    per_device_train_batch_size=1,
    gradient_accumulation_steps=4,
    num_train_epochs=3,
    learning_rate=1e-4,                  # ‚Üê –ø–æ–Ω–∏–∂–µ lr, —á—Ç–æ–±—ã —É–±—Ä–∞—Ç—å –Ω–µ—Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å
    fp16=True,                           # ‚Üê –≤–∫–ª—é—á–∞–µ–º fp16
    bf16=False,                          # ‚Üê –≤—ã–∫–ª—é—á–∞–µ–º bf16
    optim="paged_adamw_8bit",
    logging_steps=10,
    save_steps=200,
    save_total_limit=2,
    report_to="none",
)


# === 8. Trainer ===
trainer = SFTTrainer(
    model=model,
    tokenizer=tokenizer,
    train_dataset=dataset["train"],
    args=args,
    max_seq_length=768,
    formatting_func=formatting_func,
    packing=False,
)

# === 9. –ó–∞–ø—É—Å–∫ ===
if __name__ == "__main__":
    print("üöÄ –ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ Gemma 2 LoRA (4-bit)...")
    trainer.train()
    print("‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ! –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤:", OUTPUT_DIR)
